{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "709adabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.vgg16(pretrained=True).features\n",
    "\n",
    "print(backbone)\n",
    "\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f477b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae83f0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(512, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(512, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb51e698",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save() got an unexpected keyword argument 'format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./yolov5/models/hub/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: save() got an unexpected keyword argument 'format'"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(),'./yolov5/models/hub/',format='pt') # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3428ccf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1660621668.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [68]\u001b[0;36m\u001b[0m\n\u001b[0;31m    model_reload=torch.load('./yolov5/models/hub/model_faster_rcnn_vgg16.yaml',format=)\u001b[0m\n\u001b[0m                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model_reload=torch.load('./yolov5/models/hub/model_faster_rcnn_vgg16.yaml')\n",
    "model_reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cde05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20938aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pathlib\n",
    "from model import create_model\n",
    "from config import (\n",
    "    NUM_CLASSES, DEVICE, CLASSES\n",
    ")\n",
    "# construct the argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '-i', '--input', help='path to input video',\n",
    "    default='data/uno_custom_test_data/video_1.mp4'\n",
    ")\n",
    "args = vars(parser.parse_args())\n",
    "# this will help us create a different color for each class\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "# load the best model and trained weights\n",
    "model = create_model(num_classes=NUM_CLASSES)\n",
    "checkpoint = torch.load('outputs/best_model.pth', map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(DEVICE).eval()\n",
    "# define the detection threshold...\n",
    "# ... any detection having score below this will be discarded\n",
    "detection_threshold = 0.8\n",
    "RESIZE_TO = (512, 512)\n",
    "cap = cv2.VideoCapture(args['input'])\n",
    "if (cap.isOpened() == False):\n",
    "    print('Error while trying to read video. Please check path again')\n",
    "# get the frame width and height\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "save_name = str(pathlib.Path(args['input'])).split(os.path.sep)[-1].split('.')[0]\n",
    "# define codec and create VideoWriter object \n",
    "out = cv2.VideoWriter(f\"inference_outputs/videos/{save_name}.mp4\", \n",
    "                      cv2.VideoWriter_fourcc(*'mp4v'), 30, \n",
    "                      RESIZE_TO)\n",
    "frame_count = 0 # to count total frames\n",
    "total_fps = 0 # to get the final frames per second\n",
    "# read until end of video\n",
    "while(cap.isOpened()):\n",
    "    # capture each frame of the video\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame = cv2.resize(frame, RESIZE_TO)\n",
    "        image = frame.copy()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        # make the pixel range between 0 and 1\n",
    "        image /= 255.0\n",
    "        # bring color channels to front\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        # convert to tensor\n",
    "        image = torch.tensor(image, dtype=torch.float).cuda()\n",
    "        # add batch dimension\n",
    "        image = torch.unsqueeze(image, 0)\n",
    "        # get the start time\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            # get predictions for the current frame\n",
    "            outputs = model(image.to(DEVICE))\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # get the current fps\n",
    "        fps = 1 / (end_time - start_time)\n",
    "        # add `fps` to `total_fps`\n",
    "        total_fps += fps\n",
    "        # increment frame count\n",
    "        frame_count += 1\n",
    "        \n",
    "        # load all detection to CPU for further operations\n",
    "        outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "        # carry further only if there are detected boxes\n",
    "        if len(outputs[0]['boxes']) != 0:\n",
    "            boxes = outputs[0]['boxes'].data.numpy()\n",
    "            scores = outputs[0]['scores'].data.numpy()\n",
    "            # filter out boxes according to `detection_threshold`\n",
    "            boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "            draw_boxes = boxes.copy()\n",
    "            # get all the predicited class names\n",
    "            pred_classes = [CLASSES[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "            \n",
    "            # draw the bounding boxes and write the class name on top of it\n",
    "            for j, box in enumerate(draw_boxes):\n",
    "                class_name = pred_classes[j]\n",
    "                color = COLORS[CLASSES.index(class_name)]\n",
    "                cv2.rectangle(frame,\n",
    "                            (int(box[0]), int(box[1])),\n",
    "                            (int(box[2]), int(box[3])),\n",
    "                            color, 2)\n",
    "                cv2.putText(frame, class_name, \n",
    "                            (int(box[0]), int(box[1]-5)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, \n",
    "                            2, lineType=cv2.LINE_AA)\n",
    "        cv2.putText(frame, f\"{fps:.1f} FPS\", \n",
    "                    (15, 25),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), \n",
    "                    2, lineType=cv2.LINE_AA)\n",
    "        cv2.imshow('image', frame)\n",
    "        out.write(frame)\n",
    "        # press `q` to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "# release VideoCapture()\n",
    "cap.release()\n",
    "# close all frames and video windows\n",
    "cv2.destroyAllWindows()\n",
    "# calculate and print the average FPS\n",
    "avg_fps = total_fps / frame_count\n",
    "print(f\"Average FPS: {avg_fps:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
